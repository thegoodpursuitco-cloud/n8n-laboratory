[35m.git_bak_20251028_131543/hooks/fsmonitor-watchman.sample[m[36m:[m[32m91[m[36m:[m		$last_update_[1;31mtoken =[m "\"$last_update_token\"";
[35m.git_bak_20251028_131543/hooks/fsmonitor-watchman.sample[m[36m:[m[32m151[m[36m:[m		$last_update_[1;31mtoken =[m $o->{clock};
[35mcontent_automation/0752_HTTP_Rssfeedread_Automation_Scheduled.json[m[36m:[m[32m25[m[36m:[m        "url": "=https://{HERE YOUR MASTONDON INSTANCE URL}/api/v1/statuses?access_[1;31mtoken=[m{HERE YOUR MASTODON ACCESS TOKEN}",
[35mintegrations/0649_Splitout_GoogleCalendar_Create_Webhook.json[m[36m:[m[32m393[m[36m:[m        "content": "## 3.4 Scraping LinkedIn With [Apify.com](https://www.[1;31mapify[m.com?fpr=414q6)\n[Learn more about Apify.com for Web Scraping](https://www.[1;31mapify[m.com?fpr=414q6)\n\nTo get the attendee's recent LinkedIn activity, we'll need a webscraper capable of rendering the user's LinkedIn profile. We'll use [Apify.com](https://www.[1;31mapify[m.com?fpr=414q6) which is a commercial web scraping service but has a very generous monthly free tier ($5/mo).\n\nWhile Apify offers a number of dedicated LinkedIn scrapers, we'll build our own which works by impersonating our own LinkedIn account using our login cookie - this can be obtained by inspecting network requests when logged into Linkedin. **Add your LinkedIn Cookie to the node below!**"
[35mintegrations/0649_Splitout_GoogleCalendar_Create_Webhook.json[m[36m:[m[32m996[m[36m:[m      "name": "[1;31mAPIFY[m Web Scraper",
[35mintegrations/0649_Splitout_GoogleCalendar_Create_Webhook.json[m[36m:[m[32m1003[m[36m:[m        "url": "https://api.[1;31mapify[m.com/v2/acts/[1;31mapify[m~web-scraper/run-sync-get-dataset-items",
[35mintegrations/0649_Splitout_GoogleCalendar_Create_Webhook.json[m[36m:[m[32m1005[m[36m:[m        "jsonBody": "={\n  \"startUrls\": [\n    {\n      \"url\": \"{{ $json.linkedin_profile_url }}\",\n      \"method\": \"GET\"\n    }\n  ],\n  \"initialCookies\": [\n    {\n      \"name\": \"li_at\",\n      \"value\": \"{{ $json.linkedin_cookies.match(/li_at=([^;]+)/)[1] }}\",\n      \"domain\": \".www.linkedin.com\"\n    }\n  ],\n  \"breakpointLocation\": \"NONE\",\n  \"browserLog\": false,\n  \"closeCookieModals\": false,\n  \"debugLog\": false,\n  \"downloadCss\": false,\n  \"downloadMedia\": false,\n  \"excludes\": [\n    {\n      \"glob\": \"/**/*.{png,jpg,jpeg,pdf}\"\n    }\n  ],\n  \"headless\": true,\n  \"ignoreCorsAndCsp\": false,\n  \"ignoreSslErrors\": false,\n  \n  \"injectJQuery\": true,\n  \"keepUrlFragments\": false,\n  \"linkSelector\": \"a[href]\",\n  \"maxCrawlingDepth\": 1,\n  \"maxPagesPerCrawl\": 1,\n  \"maxRequestRetries\": 1,\n  \"maxResultsPerCrawl\": 1,\n  \"pageFunction\": \"// The function accepts a single argument: the \\\"context\\\" object.\\n// For a complete list of its properties and functions,\\n// see https://[1;31mapify[m.com/[1;31mapify[m/web-scraper#page-function \\nasync function pageFunction(context) {\\n\\n    await new Promise(res => { setTimeout(res, 6000) });\\n    // This statement works as a breakpoint when you're trying to debug your code. Works only with Run mode: DEVELOPMENT!\\n    // debugger; \\n\\n    // jQuery is handy for finding DOM elements and extracting data from them.\\n    // To use it, make sure to enable the \\\"Inject jQuery\\\" option.\\n    const $ = context.jQuery;\\n    const title = $('title').first().text();\\n\\n    // Clone the body to avoid modifying the original content\\n    const bodyClone = $('body').clone();\\n    bodyClone.find('iframe, img, script, style, object, embed, noscript, svg, video, audio').remove();\\n    const body = bodyClone.html();\\n\\n    // Return an object with the data extracted from the page.\\n    // It will be stored to the resulting dataset.\\n    return {\\n        url: context.request.url,\\n        title,\\n        body\\n    };\\n}\",\n  \"postNavigationHooks\": \"// We need to return array of (possibly async) functions here.\\n// The functions accept a single argument: the \\\"crawlingContext\\\" object.\\n[\\n    async (crawlingContext) => {\\n        // ...\\n    },\\n]\",\n  \"preNavigationHooks\": \"// We need to return array of (possibly async) functions here.\\n// The functions accept two arguments: the \\\"crawlingContext\\\" object\\n// and \\\"gotoOptions\\\".\\n[\\n    async (crawlingContext, gotoOptions) => {\\n        // ...\\n    },\\n]\\n\",\n  \"proxyConfiguration\": {\n    \"useApifyProxy\": true\n  },\n  \"runMode\": \"PRODUCTION\",\n  \n  \"useChrome\": false,\n  \"waitUntil\": [\n    \"domcontentloaded\"\n  ],\n  \"globs\": [],\n  \"pseudoUrls\": [],\n  \"proxyRotation\": \"RECOMMENDED\",\n  \"maxConcurrency\": 50,\n  \"pageLoadTimeoutSecs\": 60,\n  \"pageFunctionTimeoutSecs\": 60,\n  \"maxScrollHeightPixels\": 5000,\n  \"customData\": {}\n}",
[35mintegrations/0649_Splitout_GoogleCalendar_Create_Webhook.json[m[36m:[m[32m1502[m[36m:[m    "[1;31mAPIFY[m Web Scraper": {
[35mintegrations/0649_Splitout_GoogleCalendar_Create_Webhook.json[m[36m:[m[32m1690[m[36m:[m            "node": "[1;31mAPIFY[m Web Scraper",
[35mintegrations/0711_Schedule_Slack_Create_Scheduled.json[m[36m:[m[32m45[m[36m:[m        "url": "=https://api.[1;31mapify[m.com/v2/acts/arlusm~upwork-scraper-with-fresh-job-posts/run-sync-get-dataset-items",
[35mintegrations/0711_Schedule_Slack_Create_Scheduled.json[m[36m:[m[32m205[m[36m:[m        "content": "## Setup\n1. Add MongoDB, Slack credentials\n2. Add a query auth credential where the key='token' and the value being your [1;31mapify[m token\n3. Modify the 'Assign parameters' node to include the Upwork URLs you want to query for"
[35mintegrations/0916_Telegram_Gmail_Create_Triggered.json[m[36m:[m[32m342[m[36m:[m        "text": "=Do you think the following email requires me to create a response or not? Your answer should be Y if yes, or N if not. Format your answer as a JSON as either { response: Y } or { response: N } Do not add anything else to your answer at all.\n\nCriteria for emails that require a reply:\n- Direct questions or requests for information, action, or confirmation.\n- Messages seeking clarification or feedback.\n- Invitations to meetings or events that need a confirmation or rejection.\n- Emails indicating follow-up is expected or explicitly asking for a reply.\n- Client/customer queries or feedback that require acknowledgment.\n- Personal emails from somebody who might be my friend\n\nCriteria for emails that do not require a reply:\n- The email address contains \"no-reply\" or \"noreply\"!\n- Informational or update emails with no explicit call for action or response.\n- Automated notifications (e.g., system alerts, newsletters, etc.).\n- CC/BCC emails where no direct response is expected.\nReplies that only acknowledge receipt (e.g., \"Thank you,\" \"Noted\").\n\nExamples:\n\nInput:\nFrom - \"Nutricals Men's Wellness\" <store+67435528508@g.shopifyemail.com>\nSubject - Copy of British Men Swear By This\nBody: Are You Ready to Try? 🚀\\n ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏ ͏­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \\nFree Next Day Delivery Available In The UK\\n\\nBanner with 15% text\\n[https://cdn.shopify.com/s/files/1/0674/3552/8508/files/15OFF_Banner.jpg?v=1728477865]\\nhttps://nutricalsupps.co.uk/_t/c/A1020005-17FCCB1E01B626BC-1AA5C384?l=AADVDennHNHTkxTcdbVb1tb0k0%2FTC%2Fc8r1oThucVEB90pLLtSPl7AIQ1Pb9xXOddTdS0LwJxcVWIvdCayYts30R9tSMtSJ%2BPvLGsZZseeSGN2rHePGVqDYgtLuJs